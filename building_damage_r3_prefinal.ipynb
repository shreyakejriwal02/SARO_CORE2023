{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- COLLECT DATA FROM OPEN STREET MAPS---\n",
    "\n",
    "def assign_bldg_type(buildings_gdf):\n",
    "    #give buildings area_m2 attribute\n",
    "    buildings_gdf = buildings_gdf.to_crs(epsg=32637)\n",
    "    buildings_gdf['area_m2']= buildings_gdf['geometry'].area\n",
    "    graph_bldg_flat = buildings_gdf.reset_index()\n",
    "    print(graph_bldg_flat.index)\n",
    "\n",
    "    #FOR NOW WE RANDOMLY ASSIGN ATTRIBUTES TO THE CHOSEN NEIGHBOURHOOD IN GAZIANTEP BASED ON ESRM DATA TO CREATE A FAKE BUILDING ATTRIBUTE DATASET\n",
    "    #START WITH COMPUTING THE DISTRIBUTION OF BUILDING TYPES IN GAZIANTEP BASED ON ESRM DATA\n",
    "    exposure_res_turkey= pd.read_excel(\"Exposure_Model_Turkey_Res.xlsx\")\n",
    "    exposure_com_turkey= pd.read_excel(\"Exposure_Model_Turkey_Com.xlsx\")\n",
    "    exposure_ind_turkey= pd.read_excel(\"Exposure_Model_Turkey_Ind.xlsx\")\n",
    "\n",
    "    #total number of residential buildings in Gaziantep:\n",
    "    condition1= exposure_res_turkey['NAME_1']=='Gaziantep'\n",
    "    gazresbldgs= int(sum(exposure_res_turkey[condition1].BUILDINGS))\n",
    "    print (gazresbldgs)\n",
    "\n",
    "    #total number of commercial buildings in Gaziantep:\n",
    "    condition2= exposure_com_turkey['NAME_1']=='Gaziantep'\n",
    "    gazcombldgs= int(sum(exposure_com_turkey[condition2].BUILDINGS))\n",
    "    print (gazcombldgs)\n",
    "\n",
    "    #total number of industrial buildings in Gaziantep:\n",
    "    condition3= exposure_ind_turkey['NAME_1']=='Gaziantep'\n",
    "    gazindbldgs= int(sum(exposure_ind_turkey[condition3].BUILDINGS))\n",
    "    print (gazindbldgs)\n",
    "\n",
    "    #percentage of residential, commercial and industrial buildings in Gaziantep:\n",
    "    totalbldgs= gazresbldgs+gazcombldgs+gazindbldgs\n",
    "    percent_gazres= gazresbldgs/totalbldgs\n",
    "    percent_gazcom= gazcombldgs/totalbldgs\n",
    "    percent_gazind= gazindbldgs/totalbldgs\n",
    "    0\n",
    "    print(percent_gazres, percent_gazcom,percent_gazind)\n",
    "\n",
    "    #NOW BASED ON THESE PROPORTIONS WE CAN PROPORTIONALLY ASSIGN OUR GAZIANTEP BUILDING STOCK INTO RESIDENTIAL COMMERCIAL AND INDUSTRIAL TYPES\n",
    "    #CLASS FOR BUILDING AND DEFINE ATTRIBUTES BASED ON THE RELEVANT PARAMETERS FOR BUILDING CODE CLASSIFICATION\n",
    "\n",
    "    class Gaz_bldg:\n",
    "        def __init__ (self_Gaz_bldg, serial, occupancytype, footprint, structural_system, lateral_resistance, code_compliance, height, occuday, occunight):\n",
    "            self_Gaz_bldg.serial= serial  \n",
    "            self_Gaz_bldg.occu_type= occupancytype  \n",
    "            self_Gaz_bldg.footprint= footprint\n",
    "            self_Gaz_bldg.strsys= structural_system\n",
    "            self_Gaz_bldg.latres= lateral_resistance\n",
    "            self_Gaz_bldg.codecomp= code_compliance\n",
    "            self_Gaz_bldg.height= height\n",
    "            self_Gaz_bldg.occu_day= occuday\n",
    "            self_Gaz_bldg.occu_night= occunight\n",
    "        \n",
    "    #NOW PREPARE ALL THE GEOMETRY OF THE OSM IMPORT AS OBJECTS OF CLASS GAZ_BLDG, \n",
    "    # AND ASSIGN THEM ATTRIBTES BASED ON PREVAILING GAZIANTEP PATTERNS FROM XML AND CSV DATA ANALYSIS\n",
    "\n",
    "    #ARBITRARY FOR NOW:\n",
    "    #STRUCTURAL SYSTEM FROM HAND CALCS ON THE EXCEL FILE\n",
    "    occtyp_statistical =['residential', 'commercial', 'industrial'] #occupancy type\n",
    "    occtyp_weights = [percent_gazres, percent_gazcom, percent_gazind]\n",
    "\n",
    "    strsys_statistical=['CR','MUR','W','S'] #structural system\n",
    "    strsys_weights=[0.732,0.267,0.0007,0.0003]\n",
    "\n",
    "    latres_statistical=['LFINF','LDUAL','LWAL','LFM'] #lateral forces resisting system\n",
    "    latres_weights_CR=[0.209, 0.208, 0.583, 0.000]\n",
    "    latres_weights_MUR=[0.0, 0.0, 1.0, 0.0]\n",
    "    latres_weights_W=[0.0, 0.0, 1.0, 0.0]\n",
    "    latres_weights_S=[0.0, 0.0, 0.0, 1.0]\n",
    "\n",
    "    codecomp_statistical=['CDN', 'CDL', 'CDM', 'CDH'] #building code compliance\n",
    "    codecomp_weights=[0.1, 0.3, 0.4, 0.2] # based on analysis of Gaziantep data- Approximate \n",
    "\n",
    "    ht_statistical=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    ht_weights=[0.2, 0.15, 0.15, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05] # based on analysis of Gaziantep data- Approximate \n",
    "\n",
    "    bldngs = []\n",
    "    for i in graph_bldg_flat.index:\n",
    "        occtyp_random = random.choices(occtyp_statistical,weights=occtyp_weights)[0] # here the res, commm, and ind percentages were computed as 0.9283591028593825 0.0534356187548717 0.018205278385745805        \n",
    "        strsys_random = random.choices(strsys_statistical,weights=strsys_weights)[0]        \n",
    "        strsys=strsys_random\n",
    "        \n",
    "        latres = None\n",
    "        if strsys=='CR':\n",
    "            latres=random.choices(latres_statistical,latres_weights_CR)[0]\n",
    "        elif strsys=='MUR':\n",
    "            latres=random.choices(latres_statistical,latres_weights_MUR)[0]\n",
    "        elif strsys=='W':\n",
    "            latres=random.choices(latres_statistical,[0,0,0,1])[0]\n",
    "        elif strsys==\"S\":\n",
    "            latres=random.choices(latres_statistical, latres_weights_S)[0]\n",
    "        else:\n",
    "            latres = 'stinky whoopsie'\n",
    "        \n",
    "        codecomp_random = None\n",
    "        if strsys=='CR' and latres=='LDUAL':\n",
    "            codecomp_random=random.choices(codecomp_statistical, [0, 0.3, 0.4, 0.3])[0]\n",
    "        elif strsys=='CR' and latres == 'LWAL':\n",
    "            codecomp_random=random.choices(codecomp_statistical, [0, 0.3, 0.4, 0.3])[0]\n",
    "        elif strsys=='MUR':\n",
    "            codecomp_random=random.choices(codecomp_statistical, [0, 0, 1, 0])[0]\n",
    "        elif strsys== 'S' and latres=='LFINF':\n",
    "            codecomp_random=random.choices(codecomp_statistical, [0, 0, 0.4, 0.6])[0]\n",
    "        elif strsys== 'S' and latres=='LFM':\n",
    "            codecomp_random=random.choices(codecomp_statistical, [0, 0, 0.4, 0.6])[0]       \n",
    "        elif strsys== 'S' and latres=='LWAL':\n",
    "            codecomp_random=random.choices(codecomp_statistical, [0, 0, 0.4, 0.6])[0]   \n",
    "        elif strsys== 'W':\n",
    "            codecomp_random=random.choices(codecomp_statistical, [0, 0.3, 0.4, 0.3])[0]   \n",
    "        else:   \n",
    "            codecomp_random= random.choices(codecomp_statistical,codecomp_weights)[0]\n",
    "\n",
    "\n",
    "        ht_random= None #random.choices(ht_statistical,ht_weights)[0]\n",
    "        if strsys=='CR' and latres=='LFINF': \n",
    "            ht_random= random.choices(ht_statistical,[0.3,0.2,0.2,0.1,0.1,0.1,0,0,0,0,0,0])[0]\n",
    "        elif strsys=='CR' and latres=='LFM': \n",
    "            ht_random= random.choices(ht_statistical,[0.3,0.2,0.2,0.1,0.1,0.1,0,0,0,0,0,0])[0]\n",
    "        elif strsys== \"MUR\":\n",
    "            ht_random= random.choices(ht_statistical,[0.3,0.3,0.2,0.1,0.1,0,0,0,0,0,0,0])[0]\n",
    "        elif strsys== 'W':\n",
    "            ht_random= random.choices(ht_statistical,[0.3,0.2,0.2,0.1,0.1,0.1,0,0,0,0,0,0])[0]\n",
    "        else:\n",
    "            ht_random=random.choices(ht_statistical,ht_weights)[0]\n",
    "\n",
    "        occu_day = None\n",
    "        if occtyp_random=='residential':\n",
    "            occu_day=0.0171\n",
    "        elif occtyp_random=='commercial':\n",
    "            occu_day=0.077\n",
    "        elif occtyp_random=='industrial':\n",
    "            occu_day=0.087\n",
    "        else:\n",
    "            print('we do not know how many people are in these buildings, call the census takers!')\n",
    "\n",
    "        occu_night = None\n",
    "        if occtyp_random=='residential':\n",
    "            occu_night=0.06\n",
    "        elif occtyp_random=='commercial':\n",
    "            occu_night=0.005\n",
    "        elif occtyp_random=='industrial':\n",
    "            occu_night=0.006\n",
    "        else:\n",
    "            print('we do not know how many people are in these buildings, call the census takers!')   \n",
    "\n",
    "        #now we have a statistically believable distributon of building objcts and attributes for the buildings, and we can make the set of buildings\n",
    "        bldg = Gaz_bldg(i,occtyp_random, [graph_bldg['area_m2'][i]], strsys_random, latres ,codecomp_random,ht_random, occu_day, occu_night)\n",
    "        bldngs.append(bldg)\n",
    "\n",
    "    #NOW WE CREATE A DATADRAME OF THE GENERATED SYNTHETIC DATA FOR INSPECTION AND CHECKS\n",
    "    # 1. We convert the list of objects into a dictionary\n",
    "    bldgobjects = []\n",
    "    for bldg in bldngs:\n",
    "        bldgobjects.append({\n",
    "            'occupancytype' : bldg.occu_type,\n",
    "            'Footprint' : bldg.footprint[0],\n",
    "            'structural_system' : bldg.strsys,\n",
    "            'lateral_resistance' : bldg.latres,\n",
    "            'code_compliance' : bldg.codecomp,\n",
    "            'height' : bldg.height,\n",
    "            'population day' : bldg.occu_day*bldg.footprint[0]*bldg.height,\n",
    "            'population night' : bldg.occu_night*bldg.footprint[0]*bldg.height\n",
    "        })\n",
    "    print(bldgobjects)\n",
    "    \n",
    "    # 2. we create a new dataframe and export it to xlsx\n",
    "    bldgobjects_dataframe_flat= pd.DataFrame(bldgobjects)\n",
    "    # bldgobjects_dataframe_flat.to_excel('bldgobjects_flat.xlsx', index=True)\n",
    "    \n",
    "    # # NOW WE COMBINE THE RANDOMLY ATTRIBUTED OBJECTS DATAFRAME, TO THE ORIGINAL OSM DATAFRAME TO CREATE ONE BIG DATAFRAME.\n",
    "\n",
    "    consolidated_bldgdataframe = graph_bldg_flat.merge(bldgobjects_dataframe_flat, left_index=True, right_index=True)\n",
    "    \n",
    "    return consolidated_bldgdataframe, bldgobjects_dataframe_flat, bldngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "#EARTHQUAKE INPUT DATA\n",
    "#NOW WE GET TO THE EARTHQUAKE PART: WE MAKE A CLASS- EARTHQUAKES, AND GIVE IT THE RELEVANT ATTRIBUTES\n",
    "\n",
    "\n",
    "class Earthquake:\n",
    "    def __init__ (self, accg, pga, sa03, sa06, sa10, time):\n",
    "        self.accg= accg #acceleration by Gravity\n",
    "        self.pga= pga\n",
    "        self.sa03= sa03\n",
    "        self.sa06= sa06\n",
    "        self.sa10= sa10\n",
    "        self.time= time\n",
    "\n",
    "#we can now input some earthquake presets:\n",
    "\n",
    "Turkey_Feb2023_Quake= Earthquake(9.8, 0.49, 0.98, 0.98, 1.2, 400)\n",
    "#can add some more earthquakes here\n",
    "Fiction_Quake= Earthquake(9.8, 0.49, 0.98, 0.98, 1.2, 1200)\n",
    "Fiction_Quake2= Earthquake(9.8, 1, 2, 2, 2.4, 200)\n",
    "equake= Turkey_Feb2023_Quake\n",
    "print(Fiction_Quake.time)\n",
    "\n",
    "#have to add some others here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "medbetatable= pd.read_excel(\"core- fragility values simplified.xlsx\", sheet_name='Final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'consolidated_dataframe.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7608\\589862060.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[0mconsolidated_bldgdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'median_ds5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmedian_ds5_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mconsolidated_bldgdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'beta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbeta_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[0mconsolidated_bldgdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'consolidated_dataframe.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bsmeekes\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2343\u001b[0m             \u001b[0minf_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minf_rep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2344\u001b[0m         )\n\u001b[1;32m-> 2345\u001b[1;33m         formatter.write(\n\u001b[0m\u001b[0;32m   2346\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2347\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bsmeekes\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    886\u001b[0m             \u001b[1;31m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[1;31m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m             writer = ExcelWriter(  # type: ignore[abstract]\n\u001b[0m\u001b[0;32m    889\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\bsmeekes\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Append mode is not supported with xlsxwriter!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bsmeekes\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[0;32m   1105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1107\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\bsmeekes\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'consolidated_dataframe.xlsx'"
     ]
    }
   ],
   "source": [
    "#FRAGILITY ATTRIBUTES\n",
    "\n",
    "#The Fragility attributes for a given building (object) are identified by LIST COMPREHENSION, \n",
    "#reading off the object atributes and using it to sort data from ESRM vulnerability databases\n",
    "\n",
    "# Create a list to store the median and beta values for each building\n",
    "im_values = []\n",
    "imtype_values =[]\n",
    "median_ds2_values = []\n",
    "median_ds3_values = []\n",
    "median_ds4_values = []\n",
    "median_ds5_values = []\n",
    "beta_values=[]\n",
    "\n",
    "# Assuming bldngs is a list of buildings with attributes\n",
    "for bldg in bldngs:\n",
    "    # Create a mask for the current building\n",
    "    mask = (medbetatable['strsys'] == bldg.strsys) & \\\n",
    "           (medbetatable['latres'] == bldg.latres) & \\\n",
    "           (medbetatable['codecomp'] == bldg.codecomp) & \\\n",
    "           (medbetatable['height'] == bldg.height)\n",
    "    \n",
    "    # Use the mask to get the dominant IM and Median values for the current building\n",
    "    im= medbetatable.loc[mask,'IMT'].values\n",
    "    for i in im:\n",
    "        if i == 'PGA':\n",
    "            imvar= equake.pga\n",
    "        elif i== 'SA(0.3)':\n",
    "            imvar= equake.sa03\n",
    "        elif i== 'SA(0.6)':\n",
    "            imvar= equake.sa06\n",
    "        elif i== 'SA(1.0)':\n",
    "            imvar= equake.sa10\n",
    "        else:\n",
    "            print ('error retrieving intensity measure!')\n",
    "            \n",
    "    if len(im) > 0:\n",
    "        im_values.append(imvar)\n",
    "\n",
    "    imtype= medbetatable.loc[mask,'IMT'].values\n",
    "    for i in imtype:\n",
    "        if i == 'PGA':\n",
    "            imvartype= 'PGA'\n",
    "        elif i== 'SA(0.3)':\n",
    "            imvartype= 'SA(0.3)'\n",
    "        elif i== 'SA(0.6)':\n",
    "            imvartype= 'SA(0.6)'\n",
    "        else:\n",
    "            imvartype= 'SA(1.0)'\n",
    "            \n",
    "    if len(imtype) > 0:\n",
    "        imtype_values.append(imvartype)\n",
    "\n",
    "    median_ds2 = medbetatable.loc[mask, 'Median_DS1'].values\n",
    "    if len(median_ds2) > 0:\n",
    "        median_ds2_values.append(median_ds2[0])\n",
    "    median_ds3 = medbetatable.loc[mask, 'Median_DS2'].values\n",
    "    if len(median_ds3) > 0:\n",
    "        median_ds3_values.append(median_ds3[0])        \n",
    "    median_ds4 = medbetatable.loc[mask, 'Median_DS3'].values\n",
    "    if len(median_ds4) > 0:\n",
    "        median_ds4_values.append(median_ds4[0])\n",
    "    median_ds5 = medbetatable.loc[mask, 'Median_DS4'].values\n",
    "    if len(median_ds5) > 0:\n",
    "        median_ds5_values.append(median_ds5[0])\n",
    "    beta = medbetatable.loc[mask, 'Beta'].values\n",
    "    if len(beta) > 0:\n",
    "        beta_values.append(beta[0])        \n",
    "\n",
    "\n",
    "# CHeck- Print 'Median_DS2' values for each building\n",
    "# print(list(median_ds2_values))\n",
    "\n",
    "#ok so it works\n",
    "\n",
    "\n",
    "# now we append the various mediand and beta columns to the geodataframe\n",
    "\n",
    "consolidated_bldgdataframe['IM_dom']= (im_values)\n",
    "consolidated_bldgdataframe['IMtype']= (imtype_values)\n",
    "consolidated_bldgdataframe['median_ds2']= (median_ds2_values)\n",
    "consolidated_bldgdataframe['median_ds3']= (median_ds3_values)\n",
    "consolidated_bldgdataframe['median_ds4']= (median_ds4_values)\n",
    "consolidated_bldgdataframe['median_ds5']= (median_ds5_values)\n",
    "consolidated_bldgdataframe['beta']= (beta_values)\n",
    "consolidated_bldgdataframe.to_excel('consolidated_dataframe.xlsx', index=True)\n",
    "\n",
    "\n",
    "#now calculate the probability of damage states\n",
    "\n",
    "\n",
    "e= 2.71828\n",
    "pi= math.pi\n",
    "\n",
    "def p_ds2_compute(row):\n",
    "    val1= row['median_ds2'] # the median value\n",
    "    mean= np.log(val1) #mean median relation\n",
    "    val2= row['beta'] # the beta (logstandard deviation)\n",
    "    IM= row['IM_dom'] #goes in the x axis\n",
    "    lognorm_dist= stats.lognorm(scale=np.exp(mean), s=val2)\n",
    "\n",
    "    result= lognorm_dist.pdf(IM)\n",
    "    return result\n",
    "\n",
    "def p_ds3_compute(row):\n",
    "    val1= row['median_ds3'] # the median value\n",
    "    mean= np.log(val1) #mean median relation\n",
    "    val2= row['beta'] # the beta (logstandard deviation)\n",
    "    IM= row['IM_dom'] #goes in the x axis\n",
    "    lognorm_dist= stats.lognorm(scale=np.exp(mean), s=val2)\n",
    "\n",
    "    result= lognorm_dist.pdf(IM)\n",
    "    return result\n",
    "def p_ds4_compute(row):\n",
    "    val1= row['median_ds4'] # the median value\n",
    "    mean= np.log(val1) #mean median relation\n",
    "    val2= row['beta'] # the beta (logstandard deviation)\n",
    "    IM= row['IM_dom'] #goes in the x axis\n",
    "    lognorm_dist= stats.lognorm(scale=np.exp(mean), s=val2)\n",
    "\n",
    "    result= lognorm_dist.pdf(IM)\n",
    "    return result\n",
    "def p_ds5_compute(row):\n",
    "    val1= row['median_ds5'] # the median value\n",
    "    mean= np.log(val1) #mean median relation\n",
    "    val2= row['beta'] # the beta (logstandard deviation)\n",
    "    IM= row['IM_dom'] #goes in the x axis\n",
    "    lognorm_dist= stats.lognorm(scale=np.exp(mean), s=val2)\n",
    "\n",
    "    result= lognorm_dist.pdf(IM)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "#append results\n",
    "consolidated_bldgdataframe['p_ds2'] = consolidated_bldgdataframe.apply(p_ds2_compute, axis =1)\n",
    "consolidated_bldgdataframe['p_ds3'] = consolidated_bldgdataframe.apply(p_ds3_compute, axis =1)\n",
    "consolidated_bldgdataframe['p_ds4'] = consolidated_bldgdataframe.apply(p_ds4_compute, axis =1)\n",
    "consolidated_bldgdataframe['p_ds5'] = consolidated_bldgdataframe.apply(p_ds5_compute, axis =1)\n",
    "\n",
    "def p_ds5_collapse_compute(row):\n",
    "    val1= row['p_ds5']\n",
    "    result= row['p_ds5']*0.1\n",
    "    return result\n",
    "\n",
    "consolidated_bldgdataframe['p_ds5_collapse'] = consolidated_bldgdataframe.apply(p_ds5_collapse_compute, axis =1)\n",
    "consolidated_bldgdataframe.to_excel('consolidated_dataframe.xlsx', index=True)\n",
    "\n",
    "# consolidated_bldgdataframe.to_excel('C:\\\\TUD coursework\\\\0 CORE\\\\Final script\\\\scrap\\\\consolidated_dataframe.xlsx', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "night\n"
     ]
    }
   ],
   "source": [
    "#now we calculate the actualy injury numbers\n",
    "\n",
    "\n",
    "ds1_numbers=[]\n",
    "ds2_numbers=[]\n",
    "ds3_numbers=[]\n",
    "ds4_numbers=[]\n",
    "ds5_numbers=[]\n",
    "\n",
    "for bldg in bldngs:\n",
    "    mask = (medbetatable['strsys'] == bldg.strsys) & \\\n",
    "            (medbetatable['latres'] == bldg.latres) & \\\n",
    "            (medbetatable['codecomp'] == bldg.codecomp) & \\\n",
    "            (medbetatable['height'] == bldg.height)   \n",
    "    pds1n = [float(x) for x in medbetatable.loc[mask, 'injury severity DS1'].values[0].split(',')]\n",
    "    pds2n = [float(x) for x in medbetatable.loc[mask, 'injury severity DS2'].values[0].split(',')]\n",
    "    pds3n = [float(x) for x in medbetatable.loc[mask, 'injury severity DS3'].values[0].split(',')]\n",
    "    pds4n = [float(x) for x in medbetatable.loc[mask, 'injury severity DS4'].values[0].split(',')]\n",
    "    pds5n = [float(x) for x in medbetatable.loc[mask, 'injury severity DS5+collapse'].values[0].split(',')]\n",
    "    ds1_numbers.append([pds1n])\n",
    "    ds2_numbers.append([pds2n])\n",
    "    ds3_numbers.append([pds3n])\n",
    "    ds4_numbers.append([pds4n])\n",
    "    ds5_numbers.append([pds5n])\n",
    "consolidated_bldgdataframe['ds1numbers'] = ds1_numbers\n",
    "consolidated_bldgdataframe['ds2numbers'] = ds2_numbers\n",
    "consolidated_bldgdataframe['ds3numbers'] = ds3_numbers\n",
    "consolidated_bldgdataframe['ds4numbers'] = ds4_numbers\n",
    "consolidated_bldgdataframe['ds5numbers'] = ds5_numbers\n",
    "consolidated_bldgdataframe.to_excel('consolidated_dataframe.xlsx', index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def injury_profile_compute_day(row):\n",
    "    popday= row['population day']\n",
    "    popnight= row['population night']\n",
    "    pds2 = row['p_ds2']\n",
    "    pds3 = row['p_ds3']\n",
    "    pds4 = row['p_ds4']\n",
    "    pds5 = row['p_ds5']\n",
    "    pds5col = row['p_ds5_collapse']\n",
    "    p5= random.choices([pds5, pds5col], [9,1])\n",
    "    \n",
    "    b= [popday*pds2*i for i in pds2n]\n",
    "    c= [popday*pds3*i for i in pds3n]\n",
    "    d= [popday*pds4*i for i in pds4n]\n",
    "    e= [popday*pds5*i for i in pds5n]\n",
    "\n",
    "    q= ((b[0]*pds2)+(c[0]*pds3)+(d[0]*pds4)+(e[0]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    r= ((b[1]*pds2)+(c[1]*pds3)+(d[1]*pds4)+(e[1]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    s= ((b[2]*pds2)+(c[2]*pds3)+(d[2]*pds4)+(e[2]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    t= ((b[3]*pds2)+(c[3]*pds3)+(d[3]*pds4)+(e[3]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    p= (int(popday)-q-r-s-t)\n",
    "    result= [p,q,r,s,t]\n",
    "    resultfiction= [(int(popday)-2*q-2*r-2*s-2*t),2*q,2*r,2*s,2*t] #the result values were unrealistically low, - not meaningful for part B\n",
    "    return np.round(resultfiction)\n",
    "\n",
    "def injury_profile_compute_night(row):\n",
    "    popday= row['population day']\n",
    "    popnight= row['population night']\n",
    "    pds2 = row['p_ds2']\n",
    "    pds3 = row['p_ds3']\n",
    "    pds4 = row['p_ds4']\n",
    "    pds5 = row['p_ds5']\n",
    "    pds5col = row['p_ds5_collapse']\n",
    "    p5= random.choices([pds5, pds5col], [9,1])\n",
    "    \n",
    "    b= [popnight*pds2*i for i in pds2n]\n",
    "    c= [popnight*pds3*i for i in pds3n]\n",
    "    d= [popnight*pds4*i for i in pds4n]\n",
    "    e= [popnight*pds5*i for i in pds5n]\n",
    "\n",
    "    q= ((b[0]*pds2)+(c[0]*pds3)+(d[0]*pds4)+(e[0]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    r= ((b[1]*pds2)+(c[1]*pds3)+(d[1]*pds4)+(e[1]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    s= ((b[2]*pds2)+(c[2]*pds3)+(d[2]*pds4)+(e[2]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    t= ((b[3]*pds2)+(c[3]*pds3)+(d[3]*pds4)+(e[3]*pds5))/(pds2+pds3+pds4+pds5)\n",
    "    p= (int(popnight)-q-r-s-t)\n",
    "    result2= [p,q,r,s,t]\n",
    "    resultfiction= [(int(popnight)-2*q-2*r-2*s-2*t),2*q,2*r,2*s,2*t] #the result values were unrealistically low, - not meaningful for part B    \n",
    "    return np.round(resultfiction)\n",
    "\n",
    "\n",
    "\n",
    "if 900 < equake.time < 1800:\n",
    "    consolidated_bldgdataframe['injuries'] = consolidated_bldgdataframe.apply(injury_profile_compute_day, axis =1)\n",
    "    print('day')\n",
    "elif (0 < equake.time < 899) or (1801 < equake.time < 2359):\n",
    "    consolidated_bldgdataframe['injuries'] = consolidated_bldgdataframe.apply(injury_profile_compute_night, axis =1)\n",
    "    print('night')\n",
    "else:\n",
    "    print ('arghhh')\n",
    "consolidated_bldgdataframe.to_excel('consolidated_dataframe.xlsx', index=True)\n",
    "\n",
    "\n",
    "#final datafraem from part A\n",
    "\n",
    "partA_data= consolidated_bldgdataframe\n",
    "\n",
    "# print(consolidated_bldgdataframe['injuries'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPENDIX OF THINGS\n",
    "\n",
    "############ start brent method ###########\n",
    "\n",
    "# gaz_bldg_stock=[]\n",
    "\n",
    "# for i in graph_bldg.index:\n",
    "#     ser=i[1]\n",
    "#     occtyp=occtyp_random\n",
    "#     footpr=graph_bldg.area[i]\n",
    "#     strsys=strsys_random\n",
    "#     if strsys=='CR':\n",
    "#         latres=random.choices(latres_statistical,latres_weights_CR)\n",
    "#     elif strsys=='MUR':\n",
    "#         latres=random.choices(latres_statistical,latres_weights_MUR)\n",
    "#     elif strsys=='W':\n",
    "#         latres=random.choices(latres_statistical,latres_weights_W)\n",
    "#     elif strsys==\"S\":\n",
    "#         latres=random.choices(latres_statistical, latres_weights_S)\n",
    "#     codecomp=codecomp_random\n",
    "#     ht=ht_random\n",
    "#     blabla=3000\n",
    "\n",
    "#     gaz_bldg_instance=Gaz_bldg(ser,occtyp,footpr,strsys,latres,codecomp,ht,blabla)\n",
    "#     gaz_bldg_stock.append(gaz_bldg_instance)\n",
    "\n",
    "\n",
    "# print([o.strsys for o in gaz_bldg_stock])\n",
    "\n",
    "\n",
    "##### end brent method #############\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
